{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load some libraries, set some paths and the printing options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True, threshold=np.nan)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:2.2f}'.format)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 30\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the Test Set\n",
    "df_test = pd.read_csv(os.path.join(data_dir, 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Basic Preprocessing\n",
    "\n",
    "In the first step we will start to explore the data and preprocess it a little bit. The main data frame is on v2 (this is how it comes from Kaggle), at the end we will save it in its v3, so we can reload it without having to go through this step again. What we do here is removing bad lines, changing the types of the data, removing negative entries, add the shop id to the main frame, add the day_of_the_week/day/month/year from the date, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Load the Data\n",
    "df_train = pd.read_csv(os.path.join(data_dir, 'sales_train_v2.csv'))\n",
    "df_items_and_cats = pd.read_csv(os.path.join(data_dir, 'items-translated.csv'))\n",
    "df_shops = pd.read_csv(os.path.join(data_dir, 'shops-translated.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some functions for the first preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1 - Some Functions\n",
    "def fix_returned_items(df):\n",
    "\tdays_with_returns = df[df['item_cnt_day'] < 0]['date']\n",
    "\tnbr_days_with_returns = days_with_returns.value_counts().shape[0]\n",
    "\tnbr_days = df['date'].value_counts().shape[0]\n",
    "\tprint(f'\\nThere are {nbr_days_with_returns} days with returns out of {nbr_days}.')\n",
    "\tprint(f'There are {days_with_returns.shape[0]} returned transactions out of {df_train.index.shape[0]}.')\n",
    "\tstudy_returned_items_values = False\n",
    "\tif study_returned_items_values:\n",
    "\t\tsold_vs_returns = []\n",
    "\t\tfor day in days_with_returns.unique():\n",
    "\t\t\tdf_day_neg = df[(df['date'] == day) & (df['item_cnt_day'] < 0)]\n",
    "\t\t\tdf_day_pos = df[(df['date'] == day) & (df['item_cnt_day'] > 0)]\n",
    "\t\t\tsold_vs_returns.append([df_day_pos['item_price'].dot(df_day_pos['item_cnt_day']), df_day_neg['item_price'].dot(df_day_neg['item_cnt_day'])])\n",
    "\t\t\tprint([df_day_pos['item_price'].dot(df_day_pos['item_cnt_day']), df_day_neg['item_price'].dot(df_day_neg['item_cnt_day'])])\n",
    "\n",
    "\tremove_returned_items_rows = True\n",
    "\tif remove_returned_items_rows:\n",
    "\t\tdf.drop(df[df['item_cnt_day'] < 0].index, inplace=True)\n",
    "\t\tdf.reset_index(drop=True, inplace=True)\n",
    "\t\tprint(f'Removed rows when items returned')\n",
    "\n",
    "def fix_negative_prices(df):\n",
    "\tdays_with_neg_prices = df[df['item_price'] < 0]['item_price']\n",
    "\tnbr_days_with_neg_prices = days_with_neg_prices.shape[0]\n",
    "\tprint(f'\\nThere are {nbr_days_with_neg_prices} items with negative prices out of {df.shape[0]}.')\n",
    "\n",
    "\tremove_item_with_neg_price = True\n",
    "\tif remove_item_with_neg_price:\n",
    "\t\tdf.drop(df[df['item_price'] < 0].index, inplace=True)\n",
    "\t\tdf.reset_index(drop=True, inplace=True)\n",
    "\t\tprint(f'Removed rows when item has negative price')\n",
    "\n",
    "def fix_shops_id(df):\n",
    "\tdf['shop_id'] = df['shop_id'].map(dict_duplicated_shops)\n",
    "\t# now we do the trick we talked about while creating the dictionnary, to have a nice range of shops.\n",
    "\tdf['shop_id'] = df['shop_id'].map(dict_duplicated_shops_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the auxilliary dataframe \"items and categories\". Nothing to say here. We will later add the category_id for each item into the main dataframe, as it is not there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 1 - Visualize the df_items_and_cats\n",
    "df_items_and_cats.head()\n",
    "df_items_and_cats.info()\n",
    "df_items_and_cats.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the auxilliary dataframe \"shops\". We observe that there are some duplicated shops name. We thus create a dictionnary to change the id of any duplicated shop with the first time it appears in the list (in the training and test set). Notice that 57 will become 0, 58 will become 1 and 11 will become 10, so we eliminate 11,57, 58. If we later replace 59 with the eliminated 11, then the range will go from 0 to 56 nicely without jumping any shop. We thus create a 2nd dictionnary that will do just this, and apply it after applying the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Visualize and Preprocess the df_shop\n",
    "df_shops.head()\n",
    "df_shops.info()\n",
    "df_shops.describe()\n",
    "df_shops[df_shops['shop_name_translated'].duplicated(keep=False)]  # {11:10, 57:0, 58:1}\n",
    "dict_duplicated_shops = {x:x for x in df_shops['shop_id'].unique()}\n",
    "dict_duplicated_shops[11] = 10\n",
    "dict_duplicated_shops[57] = 0\n",
    "dict_duplicated_shops[58] = 1\n",
    "dict_duplicated_shops_2 = {x:x for x in range(60)}\n",
    "dict_duplicated_shops_2[59] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Visualize the main df \n",
    "df_train.head()\n",
    "df_train.info()\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First big preprocessing of the main dataframe. \n",
    "\n",
    "We first observe that there are no Null and NA values, and that there are 6 duplicate lines, which we ignore as this is not necessarily a mistake in the dataset (different people could have bought the same day the same items)\n",
    "\n",
    "We then remove the rows with negative entries for \"item_count\". There are 1023 days / 1034 that have returned items, with 7356 items returned / 2935849 transactions. This is not that many items. When looking at the total value of the items returned, a typical day would have 10.000$ returned vs 5.000.000$ bought, so 1/500 in value returned. This is not that much, especially that all days are pretty similar in this regard, so we just remove those columns. Ignoring is probably okay too.\n",
    "\n",
    "We then look at rows with negative prices: there is 1 item with negative price, remove it.\n",
    "\n",
    "We add the category of the item on main df from the df_items_and_cats datafram, as it was not in the main dataframe.\n",
    "\n",
    "We change the id's of duplicated shops and merge them together. For this we use the dictionnary previously created.\n",
    "\n",
    "We also add the day_of_week, day, month and year, as well as remove the date in string format.\n",
    "\n",
    "We downcast the types from int64 to int16/int8 and float64 to float32, saving memory from 167+MB to 64MB, that's awesome. We create a dictionnary for the types, so from now on we open the file with the correct dtypes, following the dictionnary.\n",
    "\n",
    "Finally, we save this preprocessed dataframe as v3, so we could just load it without having to go through this preprocessing again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Step 1 - Process the main df (v2 -> v3) and test set\n",
    "#print(df_train.info())\n",
    "\n",
    "# Check for Null, NA and duplicates\n",
    "df_train.isnull().sum() #no null\n",
    "df_train.isna().sum() # no na\n",
    "df_train[df_train.duplicated(keep=False)]\n",
    "\n",
    "# fix negative values\n",
    "fix_returned_items(df_train)\n",
    "fix_negative_prices(df_train)\n",
    "\n",
    "# add category_id column\n",
    "dict_of_items = dict(zip(df_items_and_cats['item_id'], df_items_and_cats['item_category_id']))\n",
    "df_train['category_id'] = df_train['item_id'].map(dict_of_items)\n",
    "\n",
    "# Fix duplicated shops\n",
    "fix_shops_id(df_train)\n",
    "fix_shops_id(df_test)\n",
    "\n",
    "# change date in (year, month, day, day of the week)\n",
    "df_train['date'] = pd.to_datetime(df_train['date'], format=\"%d.%m.%Y\")\n",
    "df_train['day'] = df_train['date'].dt.day.astype(np.int8)\n",
    "df_train['month'] = df_train['date'].dt.month.astype(np.int8)\n",
    "df_train['year'] = df_train['date'].dt.year.astype(np.int16)\n",
    "df_train['day_of_week'] = df_train['date'].dt.day_name()\n",
    "df_train.drop(columns = 'date', inplace = True)\n",
    "\n",
    "# downcast types\n",
    "print(df_train.info())\n",
    "#for col in df_train.columns:\n",
    "#    print(col, df_train[col].max())\n",
    "df_train['date_block_num'] = df_train['date_block_num'].astype(np.int8)\n",
    "df_train['shop_id'] = df_train['shop_id'].astype(np.int8)\n",
    "df_train['category_id'] = df_train['category_id'].astype(np.int8)\n",
    "df_train['item_id'] = df_train['item_id'].astype(np.int16)\n",
    "df_train['item_price'] = df_train['item_price'].astype(np.float32)\n",
    "#df_train['item_cnt_day'].unique() # seems to all be int\n",
    "df_train['item_cnt_day'] = df_train['item_cnt_day'].astype(np.int16)\n",
    "\n",
    "print(df_train.info())\n",
    "df_train.to_csv(os.path.join(data_dir, 'sales_train_v3.csv'), index=False)\n",
    "df_test.to_csv(os.path.join(data_dir, 'test_v3.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Data Exploration and Feature engineering\n",
    "\n",
    "The initial preprocessing of the data has been done and we will explore it and try to get an edge. We load the main dataframe in its v3, which is exactly the v2 dataframe preprocessed as above. This is in case we want to skip ahead the above preprocessing. In this step we will remove outliers, look at outdated items and set their sale to 0 in the test set, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2 - Load the Data (v3 =  v2 processed, jump here and skip previous preprocessing)\n",
    "dict_types_v3 = {'date_block_num':np.int8, 'shop_id':np.int8, 'category_id':np.int8, 'item_id': np.int16, 'item_price':np.float32, 'item_cnt_day':np.int16, 'day':np.int8, 'month':np.int8, 'year': np.int16}\n",
    "df_train = pd.read_csv(os.path.join(data_dir, 'sales_train_v3.csv'), dtype = dict_types_v3)\n",
    "df_test = pd.read_csv(os.path.join(data_dir, 'test_v3.csv'), dtype = dict_types_v3)\n",
    "df_train.info()\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the Test set. Observe that not all shops and not all items from the training set are to be predicted in the test set. Moreover, the test set has 214200 entries, which is 42 shops * 5100 item types. Conversly, notice that there are 363 new items in the test set (but no new shops), so we will set their sales to 0 as we can't guess any better. For this we create a 'pred_0' column in the test set, and mark those items down in there, to set their sales to 0 after the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Visualize the test df \n",
    "df_test.head()\n",
    "df_test.info()\n",
    "df_test.describe()\n",
    "\n",
    "print(f'''There are {len(df_test['shop_id'].unique())} represented shops in the test set, over {len(df_train['shop_id'].unique())} total shops in the training set''')\n",
    "print(f'''There are {len(df_test['item_id'].unique())} represented items in the test set, over {len(df_train['item_id'].unique())} total items in the training set''')\n",
    "print(f'''There are {len(set(df_test['shop_id'].unique()) - set(df_train['shop_id'].unique()))} shops in the test set that are not in the training set''')\n",
    "new_items_id = set(df_test['item_id'].unique()) - set(df_train['item_id'].unique())\n",
    "print(f'''There are {len(new_items_id)} items id in the test set (over the whole {len(df_test['item_id'].unique())} number of item ids) that are not in the training set''')\n",
    "\n",
    "df_test['pred_0'] = np.nan\n",
    "df_test.loc[df_test[df_test['item_id'].isin(new_items_id)].index, 'pred_0'] = 0\n",
    "df_test.to_csv(os.path.join(data_dir, 'test_v4.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We now explore the number of sales per item every month. Plotting the total number of sales per month should indicate which months sell the most total of items. We see that month 12 and 24 sell the most, these are December 2013 and 2014, cleary correlated with Christmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Visualizing the total number of sales per month\n",
    "df_sales_per_item = df_train.pivot_table(index = 'item_id', columns = 'date_block_num', values = 'item_cnt_day', aggfunc=np.sum, fill_value=0)\n",
    "df_sales_per_item.head()\n",
    "df_sales_per_item.sum(axis=0).plot(legend=True, label='Total number of sales per Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a scalar product with the price of each item, we can see how much value was sold everyday in total. We can see an important difference with the number of items sold: the curve is cyclic but the trend is horizontal, v.s. cyclic with downward trend for the number of items sold. This means that even though we sell less and less items, the total value does not decrease, which can be interpreted by the fact that more expensive items are sold. Also, notice the scale where there is a factor of 10^8 to multiply by. ---> Ultimately I'm not doing anything with this info though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Visualizing the total value (in money) of sales per month\n",
    "df_train['value'] = df_train['item_cnt_day'].multiply(df_train['item_price'])\n",
    "df_sales_per_item_value = df_train.pivot_table(index = 'item_id', columns = 'date_block_num', values = 'value', aggfunc=np.sum, fill_value=0)\n",
    "df_sales_per_item_value.head()\n",
    "df_sales_per_item_value.sum(axis=0).plot(legend=True, label='Average number of sales per Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see if some items have not been sold in the last 12 months. These items are most likely outdated and will not sell anymore and we can thus predict 0 for them. We see that there are 9634 item_id's that have not sold out of 21804 total, and that 3696 unique items (not item_ids) out of the 214200 total items from the test set are outdated. This is just under 2% of the test set, where we can manually set the sales to 0 after prediction. For this, we will reuse the column labeled 'pred_0' and mark those items down (and set their sales to 0 after the prediciton)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - search for outdated items\n",
    "outdated_items = df_sales_per_item[df_sales_per_item.iloc[:,-12:].sum(axis=1) < 1].index\n",
    "print(f'Outdated item ids in training set : {len(outdated_items)} / {len(df_sales_per_item.index)}')\n",
    "print(f'''Outdated item ids in test set : {len([1 for x in df_test['item_id'].unique() if x in outdated_items])} / {len(df_test['item_id'].unique())}''')\n",
    "print(f'''Outdated total items in test set : {df_test['item_id'].isin(outdated_items).sum()} / {len(df_test['item_id'].index)}''')\n",
    "\n",
    "# now creating the column in the test set in order to set the prediction of those to 0 at the end\n",
    "df_test.loc[df_test[df_test['item_id'].isin(outdated_items)].index,'pred_0'] = 0\n",
    "df_test.to_csv(os.path.join(data_dir, 'test_v4.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now remove some outliers. \n",
    "Looking at the distribution of the number of items sold per transaction, we can remove the outliers > 800, and we see that there are only two of them. The distribution is way better without these 2 outliers.\n",
    "Similarly, we exlude the one outlier with price around 300000, and replot, the distribution looks nice without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2 - remove some outliers\n",
    "print('Quantity of items sold per transaction.')\n",
    "_ = plt.boxplot(df_train['item_cnt_day'])\n",
    "plt.show()\n",
    "print('Quantity of items sold per transaction after removing outliers.')\n",
    "df_train_no_out = df_train[df_train['item_cnt_day'] < 800]\n",
    "_ = plt.boxplot(df_train_no_out['item_cnt_day'])\n",
    "plt.show()\n",
    "\n",
    "print('Price of items.')\n",
    "_ = plt.boxplot(df_train_no_out['item_price'])\n",
    "plt.show()\n",
    "print('Price of items after removing outliers.')\n",
    "df_train_no_out_2 = df_train_no_out[df_train_no_out['item_price'] < 100000]\n",
    "_ = plt.boxplot(df_train_no_out_2['item_price'])\n",
    "\n",
    "df_train = df_train_no_out_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Interrupted. The Kaggle competition is closed and I won't be able to submit any model. Eh well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
